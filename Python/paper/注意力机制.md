### 环形卷积层

相对于一般的卷积来说的话



### 归一化

LayerNorm(channel)

- $y = \frac{x - \mu}{\sigma} * \gamma + \beta$

```
mean = 所有通道值的平均值
std = 所有通道值的标准差
归一化值 = (原始值 - mean) / (std + eps)
```

- nn.LayerNorm(channels)归一化层设计用于处理 3D 序列数据 (batch, seq_len, features)
- 但图像/点云数据通常是 4D 张量 (batch, channels, height, width)
- NOTE 将数据转为3d便于归一化，然后再回归4d,这里的归一化是希望通道间进行归一化

```python
    x_norm = to_4d(self.norm1(to_3d(x)), h, w)  # 维度变换
    x = x + self.attn(x_norm)  # 残差连接
```

相当于

```python
# 使4D CNN特征能输入序列处理模块
seq_features = to_3d(cnn_features)
transformer_out = transformer(seq_features)
cnn_features = to_4d(transformer_out, h, w)
```

应用

```python
def spatial_attention(x):
    B, C, H, W = x.shape
    x_seq = to_3d(x)  # [B, H*W, C]
    
    # 计算空间注意力
    attn = torch.softmax(x_seq @ x_seq.transpose(1,2), dim=-1)
    attn_feat = attn @ x_seq
    
    return to_4d(attn_feat, H, W)
```





GS 

```python
class GS(nn.Module):
    def __init__(self, in_channel):
        super(GS, self).__init__()
        self.in_channel = in_channel
        self.conv = nn.Sequential(
            nn.Conv2d(self.in_channel, self.in_channel, (1, 1)),
            nn.BatchNorm2d(self.in_channel),
            nn.ReLU(inplace=True),
        )

    def attention(self, w):
        w = torch.relu(torch.tanh(w)).unsqueeze(-1) 
        A = torch.bmm(w, w.transpose(1, 2))
        return A

    def graph_aggregation(self, x, w):
        B, _, N, _ = x.size() 
        with torch.no_grad():
            A = self.attention(w)
            I = torch.eye(N).unsqueeze(0).to(x.device).detach() 
            A = A + I
            D_out = torch.sum(A, dim=-1) 
            D = (1 / D_out) ** 0.5
            D = torch.diag_embed(D) 
            L = torch.bmm(D, A)
            L = torch.bmm(L, D) 
        out = x.squeeze(-1).transpose(1, 2).contiguous() 
        out = torch.bmm(L, out).unsqueeze(-1)
        out = out.transpose(1, 2).contiguous() 

        return out

    def forward(self, x, w):
        out = self.graph_aggregation(x, w)
        out = self.conv(out)
        out = x + out
        return out
```



GCN



```python
class GCN(nn.Module):
    def __init__(self, in_channel):
        super(GCN, self).__init__()
        self.in_channel = in_channel
        self.conv = nn.Sequential(
            nn.Conv2d(self.in_channel, self.in_channel, (1, 1)),
            nn.BatchNorm2d(self.in_channel),
            nn.ReLU(inplace=True),
        )

    def gcn(self, x, w):
        B, _, N, _ = x.size() 

        with torch.no_grad():
            w = torch.relu(torch.tanh(w)).unsqueeze(-1) 
            A = torch.bmm(w, w.transpose(1, 2))
            I = torch.eye(N).unsqueeze(0).to(x.device).detach() 
            A = A + I
            D_out = torch.sum(A, dim=-1) 
            D = (1 / D_out) ** 0.5
            D = torch.diag_embed(D) 
            L = torch.bmm(D, A)
            L = torch.bmm(L, D) 
        out = x.squeeze(-1).transpose(1, 2).contiguous() 
        out = torch.bmm(L, out).unsqueeze(-1)
        out = out.transpose(1, 2).contiguous() 

        return out

    def forward(self, x, w):
        out = self.gcn(x, w)
        out = self.conv(out)
        return out

```