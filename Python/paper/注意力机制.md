## 注意力

ARR（脑机）中的

```python
class Attend(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, q, k, v):

        #b, n, device = q.shape[0], q.shape[-2], q.device

        scale = q.shape[-1] ** -0.5

        # similarity
        sim = einsum("b h i d, b h j d -> b h i j", q, k) * scale


        # attention

        attn = sim.softmax(dim=-1)

        # aggregate values

        out = einsum("b h i j, b h j d -> b h i d", attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return out

# attention

class Attention(nn.Module):
    def __init__(self, dim_head, qk_rmsnorm = False, rotary_pos_emb=False):
        super().__init__()
        self.attend = Attend()
        self.qk_rmsnorm = qk_rmsnorm
        self.rotary_pos_emb = rotary_pos_emb
        self.rotary_pos_emb = RotaryEmbedding(dim_head) # rotary_emb_dim = 32


        if qk_rmsnorm:
            self.q_scale = nn.Parameter(torch.ones(dim_head))
            self.k_scale = nn.Parameter(torch.ones(dim_head))

    def forward(
        self,
        q, k, v
    ):

        seq_len = q.shape[-2]
        # rotary positional embedding with xpos for length extrapolation
        if self.qk_rmsnorm:
          #print('Normalization being done.....')

          q, k = map(l2norm, (q, k))
          q = q * self.q_scale
          k = k * self.k_scale

        if exists(self.rotary_pos_emb):
          #print('Embeeding being attached.....')
          attn_rotary_pos_emb = self.rotary_pos_emb(seq_len)
          q = apply_rotary_pos_emb(q, attn_rotary_pos_emb)
          k = apply_rotary_pos_emb(k, attn_rotary_pos_emb)

        # attention
        out = self.attend(q, k, v)

        return out
    
    
    
class CrossAttention(nn.Module):
    def __init__(
        self,
        dim,
        num_state_vectors,
        dim_head ,
        heads,
        qk_rmsnorm = False,
        rotary_pos_emb = False
    ):

        super().__init__()
        self.heads = heads                     # 8
        inner_dim = dim_head * heads           # 8*32=256
        self.state_norm = LayerNorm(dim)

        self.q_from_state = nn.Linear(dim, inner_dim, bias = False)      # (512, 256)
        self.k_from_state = nn.Linear(dim, inner_dim, bias = False)      # (512, 256)
        self.v_from_state = nn.Linear(dim, inner_dim, bias = False)      # (512, 256)

        self.to_state_cross_attn = Attention(dim_head, qk_rmsnorm = qk_rmsnorm, rotary_pos_emb=rotary_pos_emb)

        self.from_state_cross_attn = Attention(dim_head, qk_rmsnorm = qk_rmsnorm, rotary_pos_emb= rotary_pos_emb)
        self.state_out_to_gate = nn.Linear(dim, dim)


    def forward(self, q, k, v, state):

        # pre norm state for attention
        batch = q.shape[0]

        normed_states = self.state_norm(state)

        # get queries for cross attention, which they do not share, although they share key / values. another intriguing detail

        q_from_state = self.q_from_state(normed_states)
        k_from_state = self.k_from_state(normed_states)
        v_from_state = self.v_from_state(normed_states)

        # Rearranging the vectors
        q_from_state = rearrange(q_from_state, '... n (h d) -> ... h n d', h = self.heads)     # Q state shape:  torch.Size([1, 8, 199, 64])

        k_from_state = rearrange(k_from_state, '... n (h d) -> ... h n d', h = self.heads)

        v_from_state = rearrange(v_from_state, '... n (h d) -> ... h n d', h = self.heads)

        if q_from_state.ndim==3:
          q_from_state= repeat(q_from_state, 'h n d-> b h n d',  b = batch)
          k_from_state= repeat(k_from_state, 'h n d-> b h n d',  b = batch)
          v_from_state= repeat(v_from_state, 'h n d-> b h n d',  b = batch)

        # cross attend from past_query

        to_state_out = self.to_state_cross_attn(q_from_state, k, v)

        # cross attend to past states key values

        from_state_out = self.from_state_cross_attn(q, k_from_state, v_from_state)

        state_out = torch.cat((to_state_out, from_state_out), dim = -1)

        return state_out
```



### 归一化

LayerNorm(channel)

- $y = \frac{x - \mu}{\sigma} * \gamma + \beta$

```
mean = 所有通道值的平均值
std = 所有通道值的标准差
归一化值 = (原始值 - mean) / (std + eps)
```

- nn.LayerNorm(channels)归一化层设计用于处理 3D 序列数据 (batch, seq_len, features)
- 但图像/点云数据通常是 4D 张量 (batch, channels, height, width)
- NOTE 将数据转为3d便于归一化，然后再回归4d,这里的归一化是希望通道间进行归一化

```python
    x_norm = to_4d(self.norm1(to_3d(x)), h, w)  # 维度变换
    x = x + self.attn(x_norm)  # 残差连接
```

相当于

```python
# 使4D CNN特征能输入序列处理模块
seq_features = to_3d(cnn_features)
transformer_out = transformer(seq_features)
cnn_features = to_4d(transformer_out, h, w)
```

应用

```python
def spatial_attention(x):
    B, C, H, W = x.shape
    x_seq = to_3d(x)  # [B, H*W, C]
    
    # 计算空间注意力
    attn = torch.softmax(x_seq @ x_seq.transpose(1,2), dim=-1)
    attn_feat = attn @ x_seq
    
    return to_4d(attn_feat, H, W)
```

## 全局层



### GS 

```python
class GS(nn.Module):
    def __init__(self, in_channel):
        super(GS, self).__init__()
        self.in_channel = in_channel
        self.conv = nn.Sequential(
            nn.Conv2d(self.in_channel, self.in_channel, (1, 1)),
            nn.BatchNorm2d(self.in_channel),
            nn.ReLU(inplace=True),
        )

    def attention(self, w):
        w = torch.relu(torch.tanh(w)).unsqueeze(-1) 
        A = torch.bmm(w, w.transpose(1, 2))
        return A

    def graph_aggregation(self, x, w):
        B, _, N, _ = x.size() 
        with torch.no_grad():
            A = self.attention(w)
            I = torch.eye(N).unsqueeze(0).to(x.device).detach() 
            A = A + I
            D_out = torch.sum(A, dim=-1) 
            D = (1 / D_out) ** 0.5
            D = torch.diag_embed(D) 
            L = torch.bmm(D, A)
            L = torch.bmm(L, D) 
        out = x.squeeze(-1).transpose(1, 2).contiguous() 
        out = torch.bmm(L, out).unsqueeze(-1)
        out = out.transpose(1, 2).contiguous() 

        return out

    def forward(self, x, w):
        out = self.graph_aggregation(x, w)
        out = self.conv(out)
        out = x + out
        return out
```

### GCN

```python
class GCN(nn.Module):
    def __init__(self, in_channel):
        super(GCN, self).__init__()
        self.in_channel = in_channel
        self.conv = nn.Sequential(
            nn.Conv2d(self.in_channel, self.in_channel, (1, 1)),
            nn.BatchNorm2d(self.in_channel),
            nn.ReLU(inplace=True),
        )

    def gcn(self, x, w):
        B, _, N, _ = x.size() 

        with torch.no_grad():
            w = torch.relu(torch.tanh(w)).unsqueeze(-1) 
            A = torch.bmm(w, w.transpose(1, 2))
            I = torch.eye(N).unsqueeze(0).to(x.device).detach() 
            A = A + I
            D_out = torch.sum(A, dim=-1) 
            D = (1 / D_out) ** 0.5
            D = torch.diag_embed(D) 
            L = torch.bmm(D, A)
            L = torch.bmm(L, D) 
        out = x.squeeze(-1).transpose(1, 2).contiguous() 
        out = torch.bmm(L, out).unsqueeze(-1)
        out = out.transpose(1, 2).contiguous() 

        return out

    def forward(self, x, w):
        out = self.gcn(x, w)
        out = self.conv(out)
        return out

```