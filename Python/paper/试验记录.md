## 总述

> 这边我们说一下baseline的基本模型的主要几个部分：特征提取块DGCNN_MAX_Block，特征筛选块OABlock，然后全局搜索块GCN

## DGCNN

> 主要是邻近点的提取和处理

### 邻近点的提取

```python
# 获得k个邻近点的索引值
def knn(x, k):
    # x(BCN) k获取的邻近数量
    # NOTE x内积*-2 ：不同邻居之间的针对C通道进行融合，B N(参考点) N（对照点）
    # BNC@BCN -> BNN
    inner = -2*torch.matmul(x.transpose(2, 1), x) #inner[32,2000,2000]内积？
    # NOTE 这边是计算每个点的总特征值
    # sum: x^2C通道上求和
    xx = torch.sum(x**2, dim=1, keepdim=True) #xx[32,1,2000]
    # NOTE 关于sum的两个矩阵，我的理解是给inner矩阵减去参考点和对照点的总特征值（我们计算的是他们相对的值，不能有基础分数）
    # -sum+2*inner-sum.T
    pairwise_distance = -xx - inner - xx.transpose(2, 1) #distance[32,2000,2000]****记得回头看

    # 对每个点，选择距离最小的 k 个点，返回它们的索引。结果是 [batch_size, num_points, k]
    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k) [32,2000,9] [32,1000,6]

    return idx[:, :, :]

# NOTE 获取图像的特征点
#  x2：x与k个临近点之间所有C通道的差值
#  获得x和x2的结合
def get_graph_feature(x, k=20, idx=None):
    #x[32,128,2000,1],k=9
    # x[32,128,1000,1],k=6
    batch_size = x.size(0)
    num_points = x.size(2)
    # 等同于squeeze(-1)
    x = x.view(batch_size, -1, num_points) #x[32,128,2000]
    # 是否有准备进行处理的参考值
    if idx is None:
        idx_out = knn(x, k=k) #idx_out[32,2000,9]
    else:
        idx_out = idx

    device = x.device

    #NOTE  按批次为顺序标记序号
    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points

    idx = idx_out + idx_base #idx[32,2000,9] 把32个批次的标号连续了

    idx = idx.view(-1) #idx[32*2000*9] 把32个批次连在一起了 [32*1000*6]
    # B C N
    _, num_dims, _ = x.size()

    # NOTE 针对我们选取的索引进行筛选
    x = x.transpose(2, 1).contiguous() #x[32,2000,128]
    # 选取我们上个步骤筛选的邻近k点
    feature = x.view(batch_size*num_points, -1)[idx, :]# (B*N,C).(BNK)-->(BNK,C)
    feature = feature.view(batch_size, num_points, k, num_dims) #feature[32,2000,9,128] BNKC
    # 把x的通道位复制k份
    # BNC1 -->BN1C -->BNKC
    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1) #x[32,2000,9,128]
    # 把x和 x-feature做拼接（B N 2K C） --> B C N 2K
    feature = torch.cat((x, x - feature), dim=3).permute(0, 3, 1, 2).contiguous() #feature[32,256,2000,9] 图特征
    return feature
```



## PoinCN

> OA块中的一个特征点的剪枝块
>
> PointCN依赖于上下文规范化来编码全局上下文。这样一个简单的操作通过特征图的均值和方差对特征图进行了归一化，这忽略了不同点之间的潜在复杂关系，可能会妨碍整体性能。

```python
class PointCN(nn.Module):
    def __init__(self, channels, out_channels=None):
        nn.Module.__init__(self)
        # 如果没有channel的话我们就让入口转为通道
        if not out_channels:
            out_channels = channels
        # 设置剪枝为空，就是创造一个空的剪枝
        self.shot_cut = None
        # 进出的通道不同的话我们就要做2d卷积操作
        # 否则（通道一致时），可以直接用原始输入做加法。
        if out_channels != channels:
            self.shot_cut = nn.Conv2d(channels, out_channels, kernel_size=1)
        # InstanceNorm2d: 对每个样本独立进行归一化，适用于风格不一致的点云或图像块。
        # BatchNorm2d: 批量归一化，加速训练、提升稳定性。
        # ReLU: 激活函数。
        # Conv2d: 1x1 卷积，做通道转换。
        # 一个正常的卷积方法
        self.conv = nn.Sequential(
            nn.InstanceNorm2d(channels, eps=1e-3),
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, out_channels, kernel_size=1),
            nn.InstanceNorm2d(out_channels, eps=1e-3),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=1)
        )

    def forward(self, x):
        # (双归一+RELU+conv2d)*2
        out = self.conv(x)
        # 是否要做一次处理再相加
        if self.shot_cut:
            out = out + self.shot_cut(x)
        else:
            out = out + x
        return out
```

## OA块

### diff_pool

分解簇

- 卷积成K通道的S(BKN1)，然后邻居做交互X(BCN)@S.T(BNK) -->BCK1
- N个点压缩成了K个簇
- 返回x_down（BCK1）

```python
class diff_pool(nn.Module):
    def __init__(self, in_channel, output_points):
        # output_points:簇的数量
        # in_channel:输入的通道数量
        nn.Module.__init__(self)
        self.output_points = output_points
        self.conv = nn.Sequential(
            nn.InstanceNorm2d(in_channel, eps=1e-3),
            nn.BatchNorm2d(in_channel),
            nn.ReLU(),
            nn.Conv2d(in_channel, output_points, kernel_size=1))

    def forward(self, x):
        # k 簇的数量
        # 卷积 bcn1
        embed = self.conv(x)  # b*k*n*1
        # 归一化 每个管道的值 bkn
        S = torch.softmax(embed, dim=2).squeeze(3)
        # X x Softmax
        # NOTE X(bcn) x bnk -> (bck)->(bck1)
        out = torch.matmul(x.squeeze(3), S.transpose(1, 2)).unsqueeze(3)
        return out
```

### diff_unpool

将原本的信息x(BCN1)和过滤的信息x2(BCK1)做一次拼合

- 对过滤的簇x2(BCK1)和没有过滤的K通道权重S(BKN1)进行一次相乘
- 对簇单位做一次评价出N个点

```python
class diff_unpool(nn.Module):
    def __init__(self, in_channel, output_points):
        nn.Module.__init__(self)
        self.output_points = output_points
        self.conv = nn.Sequential(
            nn.InstanceNorm2d(in_channel, eps=1e-3),
            nn.BatchNorm2d(in_channel),
            nn.ReLU(),
            nn.Conv2d(in_channel, output_points, kernel_size=1))

    def forward(self, x_up, x_down):
        # x_up: b*c*n*1
        # x_down: b*c*k*1
        # softMax(IBRC(x_up))
        embed = self.conv(x_up)  # b*k*n*1
        # NOTE 相比diff_pool在N进行softmax，这次我们在C做softmax
        S = torch.softmax(embed, dim=1).squeeze(3)  # b*k*n
        # 对过滤的簇x_down和没有过滤的K通道权重S进行一次相乘
        # (BCK) @ (BKN)->(BCN)
        out = torch.matmul(x_down.squeeze(3), S).unsqueeze(3)
        return out
```

### OAFilter

在簇单位x_down（BKC1）相加前后两次卷积的处理丰富信息，然后做一次卷积获得out转为BCK1

- 返回x2 = out+x_down(BCK1)

```python
# TAG OA块
class OAFilter(nn.Module):
    def __init__(self, channels, points, out_channels=None):
        # channel原始通道
        # points簇的数量
        nn.Module.__init__(self)
        if not out_channels:
            out_channels = channels
        self.shot_cut = None
        if out_channels != channels:
            self.shot_cut = nn.Conv2d(channels, out_channels, kernel_size=1)
        self.conv1 = nn.Sequential(
            nn.InstanceNorm2d(channels, eps=1e-3),
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, out_channels, kernel_size=1),  # b*c*n*1
            trans(1, 2))
        # Spatial Correlation Layer
        self.conv2 = nn.Sequential(
            nn.BatchNorm2d(points),
            nn.ReLU(),
            nn.Conv2d(points, points, kernel_size=1)
        )

        # self.gcn = GCN_Block(points)
        # self.linear_0 = nn.Conv2d(points, 1, (1, 1))

        self.conv3 = nn.Sequential(
            trans(1, 2),
            nn.InstanceNorm2d(out_channels, eps=1e-3),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=1)
        )

    def forward(self, x):
        # BCK1
        # BCK1 -> BKC1
        out = self.conv1(x) 
        # w0 = self.linear_0(out).view(x.size(0), -1) #w0[32,2000]
        # out BKC1
        # out = IBRC(x).T + BRC(x)
        # BKC1 + BKC1
        out = out + self.conv2(out)  #  + self.gcn(out, w0.detach())
        # out = IBRC(out.T)
        # BCK1
        out = self.conv3(out)
        # 前后处理的通道不同的话我们就要做卷积处理
        if self.shot_cut:
            out = out + self.shot_cut(x)
        else:
            out = out + x
        return out
```

### OABlock

```python
class OABlock(nn.Module):
    def __init__(self, net_channels, depth=6, clusters=250):
        nn.Module.__init__(self)
        channels = net_channels
        # 层数
        self.layer_num = depth

        # l2 OAFilter块
        l2_nums = clusters
        self.down1 = diff_pool(channels, l2_nums)
        self.up1 = diff_unpool(channels, l2_nums)
        self.l2 = []
        for _ in range(self.layer_num // 2):
            self.l2.append(OAFilter(channels, l2_nums))
        self.l2 = nn.Sequential(*self.l2)

        self.output = nn.Conv2d(channels, 1, kernel_size=1)
        self.shot_cut = nn.Conv2d(channels * 2, channels, kernel_size=1)

    def forward(self, data):
        # data: b*c*n*1
        x1_1 = data 
        # 划分簇
        # bck1
        x_down = self.down1(x1_1)
        # OA
        # bcc1
        x2 = self.l2(x_down)
        # 分解簇
        x_up = self.up1(x1_1, x2)

        out = torch.cat([x1_1, x_up], dim=1)
        return self.shot_cut(out)
```

### 修改建议

chcanet的方法是做了2次的OA块的处理，

OA块中将初始记录x和处理完成的x_up拼合然后做卷积



## 全局GCN

### 师兄的版本

> attention方法中做权重内积然后求和，这个方法目前来看的话我觉得效果一般，最后的效果就是通过w的ADA得到了一个具有相同值的L作为参考矩阵，与x做矩阵乘法

### 公用的版本

> attention方法中是把每个点之间进行权重相乘形成一个BNN的矩阵记录各自之间的关系

```python
class GCN_Block(nn.Module):
    def __init__(self, in_channel):
        super(GCN_Block, self).__init__()
        self.in_channel = in_channel
        self.conv = nn.Sequential(
            nn.Conv2d(self.in_channel, self.in_channel, (1, 1)),
            nn.BatchNorm2d(self.in_channel),
            nn.ReLU(inplace=True),
        )

    # NOTE 双曲正切激活并且内积和将信息聚合到一起
    def attention(self, w):
        # BN
        # 双曲正切然后最后一排加空间
        # BN1
        w = torch.relu(torch.tanh(w)).unsqueeze(-1) #w[32,2000,1] 变成0到1的权重
        # wT x w
        # BN1 x B1N ->BNN
        # 特征内积和聚合到一个数值
        # NOTE 
        A = torch.bmm(w.transpose(1, 2), w ) #A[32,1,1]
        return A

    '''
    x与w的结合
    w处理
    + A：w双曲正切内积和
    + A+I求每行的和取倒数然后开方换成对角矩阵得D
    + L = DAD (BNN)
    结合
    + x(BCN1)->BNC
    + out=L(BNN)@X(BNC)->BNC->BCN1
    '''
    def graph_aggregation(self, x, w):
        B, _, N, _ = x.size() #B=32,N=2000
        # 全局上下文嵌入fg = ()
        # 清空数据
        with torch.no_grad():
            A = self.attention(w) #A[32,1,1]
            # 生成N*N单位矩阵
            I = torch.eye(N).unsqueeze(0).to(x.device).detach() #I[1,2000,2000]单位矩阵
            A = A + I #A[32,2000,2000]
            D_out = torch.sum(A, dim=-1) #D_out[32,2000]
            D = (1 / D_out) ** 0.5 #权的倒数再开方
            # 对角矩阵
            D = torch.diag_embed(D) #D[32,2000,2000]
            # DAD
            L = torch.bmm(D, A)
            L = torch.bmm(L, D) #L[32,2000,2000]
        # 交换成 BCNW->BNCW 每个单体的层次为单位计算
        # contiguous:确保矩阵在连续物理单元中
        out = x.squeeze(-1).transpose(1, 2).contiguous() #out[32,2000,128]
        # L(BNN) @ X(BNC)->BNC->BNC1
        out = torch.bmm(L, out).unsqueeze(-1)
        # BCN1
        out = out.transpose(1, 2).contiguous() #out[32,128,2000,1]

        return out

    def forward(self, x, w):
        #x[32,128,2000,1],w[32,2000]
        # NOTE 将我们处理后得特征点和权重进行结合
        out = self.graph_aggregation(x, w)
        # 卷积
        out = self.conv(out)
        return out
```

